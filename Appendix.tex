% Appendices
\appendix

% Reset page style for appendices
\pagestyle{plain}

% Remove appendices from Table of Contents

% Ensure "Appendix" appears in TOC
\renewcommand{\appendixname}{Appendix}


% Customize appendix chapter format
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{primary}}
  {\appendixname\ \thechapter}
  {20pt}
  {\Huge\textcolor{primary}}

\chapter*{ChatPromptTemplate for Real Estate Assistant}
\label{app:A}

The following pseudo code demonstrates the prompt engineering approach for the Real Estate Assistant:

\begin{verbatim}
ALGORITHM: ChatPromptTemplate for Real Estate Assistant
INPUT: context, question, language
OUTPUT: structured_prompt

BEGIN
    // Initialize ChatPromptTemplate
    prompt = ChatPromptTemplate.from_template(
        "Based on the following context, provide a detailed answer to the user's query.
        
        Context: {context}
        User Query: {question}
        Response Language: {language}
        
        Instructions:
        1. Extract the relevant answer from the ANSWER section in the context
        2. Present the information in a clear, structured way
        3. Use the exact information from the context without adding external knowledge
        
        If the query is general conversation (like greetings, how are you, etc.), 
        respond naturally in the specified language.
        
        If the query is about a topic covered in the context but is within general knowledge, 
        respond with the exact information from context.
        
        If the query is not related to real estate, respond with the equivalent of 
        'I'm specialized in Tunisian real estate law and regulations.'
        
        If no relevant information is found and you cannot provide a general answer, 
        respond with the equivalent of 'I don't have specific information about this topic 
        in my current knowledge base.'"
    )
    
    // Format the prompt with actual values
    formatted_prompt = prompt.format(
        context=context,
        question=question,
        language=language
    )
    
    RETURN formatted_prompt
END
\end{verbatim}

\chapter*{Voice Generation for Real Estate Assistant}
\label{app:B}

The following pseudo code demonstrates the voice generation implementation using ElevenLabs:

\begin{verbatim}
ALGORITHM: Voice Generation for Real Estate Assistant
INPUT: text_response, user_preferences, voice_settings
OUTPUT: audio_stream

BEGIN
    // Initialize ElevenLabs API connection
    api_client = initialize_elevenlabs_client(API_KEY)
    
    // Voice configuration
    voice_id = get_selected_voice_id(user_preferences)
    voice_settings = {
        stability: 0.8,
    }
    
    // Text preprocessing
    cleaned_text = preprocess_text(text_response)
    chunks = split_text_into_chunks(cleaned_text, MAX_CHUNK_SIZE)
    
    // Voice generation process
    audio_segments = []
    FOR each chunk in chunks DO
        try:
            audio_segment = api_client.generate_voice(
                text=chunk,
                voice_id=voice_id,
                model="eleven_multilingual_v2",
                voice_settings=voice_settings
            )
            audio_segments.append(audio_segment)
        catch APIException as e:
            log_error("Voice generation failed: " + e.message)
            RETURN fallback_response()
        end try
    END FOR
    
    // Combine audio segments
    final_audio = concatenate_audio_segments(audio_segments)
    
    // Post-processing
    final_audio = apply_audio_filters(final_audio)
    final_audio = normalize_volume(final_audio)
    
    RETURN final_audio
END
\end{verbatim}

\chapter*{Natural Language to SQL Processing}
\label{app:C}

The following pseudo code demonstrates the complete workflow for converting natural language queries to SQL and generating human-readable responses for the Role-Based Backoffice Agent:

\section*{Natural Language to SQL Generation}

\begin{verbatim}
FUNCTION convert_question_to_sql(user_question, user_id):
    
    // Step 1: Get user information
    user_info = get_user_role_and_store(user_id)
    role = user_info.role          // 'user', 'admin', or 'super_admin'
    agency_id = user_info.agency_id
    
    // Step 2: Create database description with permissions
    database_info = build_database_description(role, agency_id)
    
    // Step 3: Ask AI to write SQL
    prompt = "Given this database: {database_info}
              User asks: {user_question}
              Write a MySQL query that respects user permissions.
              Only return the SQL code."
    
    sql_query = ask_ai_model(prompt)
    clean_sql = remove_formatting(sql_query)
    
    // Step 4: Check if SQL is safe
    IF is_sql_safe(clean_sql, role, agency_id):
        RETURN clean_sql
    ELSE:
        RETURN "Cannot create safe query"

FUNCTION get_user_role_and_store(user_id):
    query = "SELECT role, store_id FROM users WHERE id = ?"
    result = execute_database_query(query, user_id)
    RETURN result

FUNCTION build_database_description(role, agency_id):
    description = "Tables: users, properties, agency, transactions, clients
                   
                   Access Rules:
                   - user: Can only see their agency's data
                    - admin: Can see all data in their agency  
                   - super_admin: Can see everything"
    
    IF role != 'super_admin':
        description += f"IMPORTANT: Must filter by agency_id = {agency_id}"
    
    RETURN description

FUNCTION is_sql_safe(sql, role, store_id):
    // Block dangerous operations
    dangerous_words = ['DELETE', 'DROP', 'UPDATE', 'INSERT']
    FOR word IN dangerous_words:
        IF word IN sql.upper():
            RETURN false
    
    // Check role permissions
    IF role != 'super_admin' AND 'WHERE' NOT IN sql.upper():
        RETURN false  // Must have WHERE clause
    
    RETURN true
\end{verbatim}

\section*{SQL Query Execution}

\begin{verbatim}
FUNCTION run_sql_query(sql_query):
    
    TRY:
        // Step 1: Connect to database
        connection = connect_to_database()
        
        // Step 2: Run the query with timeout
        start_time = current_time()
        results = execute_query(connection, sql_query, timeout=30)
        end_time = current_time()
        
        // Step 3: Limit results if too many
        IF length(results) > 1000:
            results = first_1000_items(results)
            results.add("... (truncated)")
        
        // Step 4: Return success
        RETURN {
            'success': true,
            'data': results,
            'execution_time': end_time - start_time
        }
    
    CATCH error:
        // Step 5: Handle errors nicely
        user_message = make_error_friendly(error)
        RETURN {
            'success': false,
            'error': user_message
        }
    
    FINALLY:
        close_database_connection(connection)

FUNCTION make_error_friendly(database_error):
    error_text = string(database_error).lower()
    
    IF 'syntax' IN error_text:
        RETURN "There's a problem with the query format"
    ELSE IF 'timeout' IN error_text:
        RETURN "Query took too long to run"
    ELSE IF 'connection' IN error_text:
        RETURN "Cannot connect to database"
    ELSE:
        RETURN "Database error occurred"
\end{verbatim}

\section*{Database Results to Natural Language}

\begin{verbatim}
FUNCTION convert_results_to_answer(query_results, user_role):
    
    // Step 1: Check if we have data
    IF query_results is empty:
        RETURN "No data found for your request"
    
    // Step 2: Choose response style based on user role
    response_style = get_response_style(user_role)
    
    // Step 3: Prepare data for AI
    simplified_data = simplify_data_for_ai(query_results)
    
    // Step 4: Ask AI to write natural response
    prompt = f"Convert this data to natural language:
              Data: {simplified_data}
              Style: {response_style}
              Keep it short and clear."
    
    natural_response = ask_ai_model(prompt)
    clean_response = clean_ai_response(natural_response)
    
    RETURN clean_response

FUNCTION get_response_style(user_role):
    styles = {
        'super_admin': "Executive summary with key insights",
        'admin': "Management report with important numbers", 
        'user': "Simple explanation in everyday language"
    }
    RETURN styles[user_role]

FUNCTION simplify_data_for_ai(results):
    // If too much data, summarize it
    IF length(results) > 50:
        summary = create_data_summary(results)
        RETURN summary
    ELSE:
        RETURN results

FUNCTION create_data_summary(large_dataset):
    summary = {
        'total_rows': length(large_dataset),
        'sample_data': first_10_rows(large_dataset),
        'key_numbers': extract_important_numbers(large_dataset)
    }
    RETURN summary
\end{verbatim}

\section*{Main Processing Flow}

\begin{verbatim}
FUNCTION process_user_request(user_question, user_id):
    
    // Step 1: Convert question to SQL
    sql_query = convert_question_to_sql(user_question, user_id)
    
    IF sql_query == "Cannot create safe query":
        RETURN "Sorry, I cannot process that request safely"
    
    // Step 2: Run the SQL query
    query_results = run_sql_query(sql_query)
    
    IF query_results.success == false:
        RETURN f"Database error: {query_results.error}"
    
    // Step 3: Convert results to natural language
    user_info = get_user_role_and_store(user_id)
    final_answer = convert_results_to_answer(query_results.data, user_info.role)
    
    // Step 4: Log the interaction (optional)
    log_interaction(user_question, sql_query, final_answer, user_id)
    
    RETURN final_answer

// Example usage
FUNCTION main():
    user_question = "How many properties do we have listed?"
    user_id = 3
    
    answer = process_user_request(user_question, user_id)
    print(answer)
\end{verbatim}

\chapter*{Hybrid Recommendation Algorithm}
\label{app:D}

The following pseudo code demonstrates the hybrid recommendation algorithm that combines user preferences, collaborative filtering, and popularity scoring:

\begin{verbatim}
FUNCTION calculate_property_score(user_id, property_id):
    user_score = get_user_preference_score(user_id, property_id)
    similar_score = get_similar_users_score(user_id, property_id)
    popularity_score = get_popularity_score(property_id)
    
    final_score = (user_score * 0.5) + (similar_score * 0.3) + (popularity_score * 0.2)
    RETURN final_score

FUNCTION get_user_preference_score(user_id, property_id):
    user = get_user(user_id)
    property = get_property(property_id)
    score = 0
    
    IF property.type == user.preferred_type: score += 30
    IF property.location == user.preferred_location: score += 20
    score += count_matching_amenities(property.amenities, user.preferred_amenities) * 2
    
    RETURN score

FUNCTION get_similar_users_score(user_id, property_id):
    similar_users = find_similar_users(user_id, limit=10)
    total_score = 0
    user_count = 0
    
    FOR each similar_user IN similar_users:
        interaction = get_user_interaction(similar_user.id, property_id)
        IF interaction exists:
            weighted_score = calculate_interaction_score(interaction) * similar_user.similarity
            total_score += weighted_score
            user_count += 1
    
    RETURN user_count > 0 ? total_score / user_count : 0

FUNCTION find_similar_users(user_id, limit):
    target_user = get_user(user_id)
    similar_users = []
    
    FOR each other_user IN get_all_users():
        IF other_user.id != user_id:
            similarity = calculate_user_similarity(target_user, other_user)
            IF similarity > 0.3:
                similar_users.add({'id': other_user.id, 'similarity': similarity})
    
    similar_users.sort_by_similarity_desc()
    RETURN similar_users.take(limit)

FUNCTION calculate_user_similarity(user1, user2):
    similarity = 0
    
    IF user1.preferred_type == user2.preferred_type: similarity += 0.3
    IF user1.preferred_location == user2.preferred_location: similarity += 0.3
    similarity += count_common_amenities(user1.amenities, user2.amenities) * 0.05
    
    RETURN min(similarity, 1.0)

FUNCTION calculate_interaction_score(interaction):
    base_scores = {"view": 10, "favorite": 30, "contact": 50, "invest": 90}
    score = base_scores[interaction.type]
    
    days_ago = days_since(interaction.timestamp)
    IF days_ago <= 7: score *= 1.2
    ELSE IF days_ago > 30: score *= 0.8
    
    RETURN score
\end{verbatim}

\chapter*{AI Approach Selection: RAG vs Fine-Tuning vs Prompt Engineering}
\label{app:E}

The development of the Real Estate Assistant required careful consideration of different AI approaches to ensure optimal performance for domain-specific legal and regulatory queries. This section analyzes the three primary approaches and justifies our selection.

\section*{Prompt Engineering}

Prompt engineering is the process of crafting input text (prompts) that guide a pre-trained language model to respond in a specific way. The purpose is to optimize the interaction between users and the model without modifying the underlying model parameters.

\textbf{Example Comparison:}
\begin{itemize}
    \item \textbf{Without prompt engineering:} User asks "What is the capital of France?" and receives a simple response: "Paris"
    \item \textbf{With prompt engineering:} The system uses a crafted prompt: "In a friendly tone, tell me: What is the capital of France?" resulting in: "Oh, that's easy! The capital of France is Paris!"
\end{itemize}

\section*{Fine-Tuning}

Fine-tuning involves training a pre-trained model using domain-specific datasets. While pre-trained models excel at general conversational abilities, they often struggle with specialized domains and may produce inaccurate responses or hallucinations when addressing intricate domain-specific questions.

\textbf{Characteristics:}
\begin{itemize}
    \item Requires substantial computational resources (high-end GPUs, large memory)
    \item Needs cleaned, labeled datasets specific to the domain
    \item Demands technical expertise in large language model training
    \item Suitable for specialized tasks with sufficient training data
\end{itemize}

\section*{RAG (Retrieval-Augmented Generation)}

RAG combines the strengths of retrieval-based and generation-based approaches by retrieving relevant information from a knowledge base and using it to generate contextually appropriate responses.

\textbf{Selected Approach:} For the Korpor Real Estate Assistant, we implemented a RAG-based solution due to its optimal balance of accuracy, cost-effectiveness, and maintainability for domain-specific legal and regulatory queries.

% Bibliography
\cleardoublepage
\pagestyle{empty} % Remove page numbers from bibliography
\renewcommand{\bibname}{Bibliography}
% \begin{thebibliography}{99}
%   % Add your bibliography entries here
%   % \bibitem{ref1} Author, A. (Year). Title of the work. Publisher.
%   % \bibitem{ref2} Author, B. (Year). Title of the work. Journal, Volume(Issue), Pages.
% \end{thebibliography} 